{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DUY\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "from ops import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to train GAN for generating faces and then we will make fun playing with it. Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”). One neural network, called the generator, generates new faces, while the other, the discriminator,  decides whether each instance of face it reviews belongs to the actual training dataset or not.\n",
    "\n",
    "Firstly download aligned faces of celebrities from here <a href=\"https://yadi.sk/d/xjuClJJH3MAVXh\">link</a> and extract them into folder near ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant variables below depends on your dataset and choosing of architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './aligned_celebA/' # Path to the dataset with celebA faces\n",
    "Z_DIM=100 # Dimension of face's manifold\n",
    "GENERATOR_DENSE_SIZE=64*8 # Length of first tensor in generator\n",
    "\n",
    "IMAGE_SIZE=64 # Shapes of input image\n",
    "IMAGE_SIZE1 = 218\n",
    "IMAGE_SIZE2 = 178\n",
    "BATCH_SIZE=64 # Batch size\n",
    "N_CHANNELS = 3 # Number channels of input image\n",
    "\n",
    "MERGE_X = 8 # Number images in merged image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(os.path.exists(DATA_PATH)), 'Please, download aligned celebA to DATA_PATH folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define GAN. To do it, we need to define generator, discriminator and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tips on the architecture of the generator:\n",
    "1. The deeper is convolution, the less filters is using.\n",
    "2. Apply deconvolutions-relu layers to achieve input image shape.\n",
    "3. Use batch normalization before nonlinearity for speed and stability of learning.\n",
    "4. Use tanh activation at the end of network (in this case images should be scaled to [-1, 1])\n",
    "5. To force generator not to collapse and produce different outputs initialize bias with zero (see linear layer).\n",
    "\n",
    "Other useful tips: https://github.com/soumith/ganhacks. Example of architecture see below. You may also use defined layers from ops.py. <b> Please, use names for layers started with \"g\\_\" for generator and \"d_\" for discriminator.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/carpedm20/DCGAN-tensorflow/master/DCGAN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing generator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, is_training):\n",
    "    # Firstly let's reshape input vector into 3-d tensor. \n",
    "    z_ = linear(z, GENERATOR_DENSE_SIZE * 4 * 4, 'g_h0_lin')\n",
    "    h_in = tf.reshape(z_, [-1, 4, 4, GENERATOR_DENSE_SIZE])\n",
    "    \n",
    "    h_1 = deconv2d(h_in, [BATCH_SIZE, 8, 8, 256], name='g_h1_deconv')\n",
    "    h_2 = tf.layers.batch_normalization(h_1, name='g_h2_batchnormalize')\n",
    "    h_3 = tf.nn.leaky_relu(h_2, name='g_h3_relu')\n",
    "    \n",
    "    h_4 = deconv2d(h_3, [BATCH_SIZE, 16, 16, 128], name='g_h4_deconv')\n",
    "    h_5 = tf.layers.batch_normalization(h_4, name='g_h5_batchnormalize')\n",
    "    h_6 = tf.nn.leaky_relu(h_5, name='g_h6_relu')\n",
    "    \n",
    "    h_7 = deconv2d(h_6, [BATCH_SIZE, 32, 32, 64], name='g_h7_deconv')\n",
    "    h_8 = tf.layers.batch_normalization(h_7, name='g_h8_batchnormalize')\n",
    "    h_9 = tf.nn.leaky_relu(h_8, name='g_h9_relu')\n",
    "                   \n",
    "    \"\"\"\n",
    "        Your code goes here.\n",
    "    \"\"\"\n",
    "    \n",
    "    h_out = deconv2d(h_9, [BATCH_SIZE, IMAGE_SIZE1, IMAGE_SIZE2, N_CHANNELS],\n",
    "            name='g_out')\n",
    "\n",
    "    return tf.nn.tanh(h_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define discriminator. Discriminator takes 3d tensor as input and outputs one number - probability that this is an image.\n",
    "\n",
    "Some advice for discriminator's architecture:\n",
    "1. Use batch normalization between convolutions and nonlinearities.\n",
    "2. Use leaky relu with the leak about 0.2.\n",
    "3. The deeper the layer, the more filters you can use.\n",
    "\n",
    "If you use batch normalization, please define every layer in their own scope and pass is_training parameter there. Or you may use class of batch normalization from ops.py. Do not forget to fratten tensor after the convolution blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing discriminator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(image, is_training, batch_norms=None):\n",
    "    \"\"\"\n",
    "        Your code goes here.\n",
    "    \"\"\"\n",
    "    d0 = tf.layers.conv2d(image, 64, (3,3), name='d_d0_conv2d')\n",
    "    d1 = tf.layers.batch_normalization(d0, name='d_d1_batchnorm')\n",
    "    d2 = tf.nn.leaky_relu(d1, name='d_d2_relu')\n",
    "    \n",
    "    d3 = tf.layers.conv2d(d2, 128, (3,3), name='d_d3_conv2d')\n",
    "    d4 = tf.layers.batch_normalization(d3, name='d_d4_batchnorm')\n",
    "    d5 = tf.nn.leaky_relu(d4, name='d_d5_relu')\n",
    "    \n",
    "    d6 = tf.layers.conv2d(d5, 256, (3,3), name='d_d6_conv2d')\n",
    "    d7 = tf.layers.batch_normalization(d6, name='d_d7_batchnorm')\n",
    "    d8 = tf.nn.leaky_relu(d7, name='d_d8_relu')\n",
    "    \n",
    "    d9 = tf.layers.conv2d(d8, 512, (3,3), name='d_d9_conv2d')\n",
    "    d10 = tf.layers.batch_normalization(d9, name='d_d10_batchnorm')\n",
    "    d11 = tf.nn.leaky_relu(d10, name='d_d11_relu')\n",
    "    \n",
    "    d12 = tf.layers.flatten(d11, name='d_d12_flatten')\n",
    "    linear_out = tf.layers.dense(d12, 1, None, name='d_d13_out')\n",
    "    \n",
    "    return tf.nn.sigmoid(linear_out), linear_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with tf.variable_scope(\"G\") as scope:\n",
    "    z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n",
    "    G = generator(z, is_training)\n",
    "\n",
    "with tf.variable_scope('D') as scope:\n",
    "    images = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS])\n",
    "    \n",
    "    # If you use batch norms from ops define them here (like batch_norms = [batch_norm(name='d_bn0')])\n",
    "    # and pass to discriminator function instances.\n",
    "    D_real, D_real_logits = discriminator(images, is_training)\n",
    "    scope.reuse_variables()\n",
    "    D_fake, D_fake_logits = discriminator(G, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write definition of loss funstions according to formulas:\n",
    "$$ D\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m}[\\log{D(x_i)} + \\log{(1 - D(G(z_i)))}]$$\n",
    "$$ G\\_loss = \\frac{1}{m} \\sum_{i=1}^{m} \\log{(1 - D(G(z_i)))}$$\n",
    "\n",
    "Or for better learning you may try other loss for generator:\n",
    "$$ G\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m} \\log{(D(G(z_i)))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing loss functions (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        You code goes here. Define discriminator and generator losses\n",
    "\"\"\"\n",
    "d_loss_real = -tf.reduce_mean(tf.log(D_real))\n",
    "\n",
    "d_loss_fake = -tf.reduce_mean(tf.log(1-D_fake))\n",
    "\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizers. We use different optimizers for discriminator and generator, so we needed a separate prefix for the discriminator and generator variables (g_ for generator, d_ for disciminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "## All variables of discriminator\n",
    "d_vars = [v for v in tvars if 'd_' in v.name]\n",
    "\n",
    "## All variables of generator\n",
    "g_vars = [v for v in tvars if 'g_' in v.name]\n",
    "\n",
    "LEARNING_RATE = 0.0002 # Learning rate for adam optimizer\n",
    "BETA = 0.5 # Beta paramater in adam optimizer\n",
    "\n",
    "##Optimizers - ypu may use your favourite instead.\n",
    "d_optim = tf.train.AdamOptimizer(LEARNING_RATE, beta1=BETA) \\\n",
    "                  .minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(LEARNING_RATE, beta1=BETA) \\\n",
    "                  .minimize(g_loss, var_list=g_vars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob(os.path.join(DATA_PATH, \"*.jpg\"))\n",
    "assert(len(data) > 0), \"Length of training data should be more than zero\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(sess, load_dir):\n",
    "    \"\"\"load network's paramaters\n",
    "    \n",
    "    load_dir : path to load dir\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(load_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN (1 point + 2 for good results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, load_dir=None, save_frequency=100, sample_frequency=100, sample_dir='sample_faces',\n",
    "          save_dir='checkpoint', max_to_keep=1, model_name='dcgan.model',\n",
    "          n_epochs=25, n_generator_update=2):\n",
    "    \"\"\"train gan\n",
    "    Parameters\n",
    "    -------------------------------------------\n",
    "    load_dir : str, default = None\n",
    "        path to the folder with parameters\n",
    "    save_frequency: int, default = 100\n",
    "        how often save parameters []\n",
    "    sample_frequency: int, default = None (not sample)\n",
    "        how often sample faces\n",
    "    sample_dir: str, default = samples\n",
    "        directory for sampled images\n",
    "    save_dir: str, default = 'checkpoint'\n",
    "        path where to save parameters\n",
    "    max_to_keep: int, default = 1\n",
    "        how many last checkpoints to store\n",
    "    model_name: str, default='dcgan.model'\n",
    "        name of model\n",
    "    n_epochs: int, default = 25 \n",
    "        number epochs to train\n",
    "    n_generator_update: int, default = 2\n",
    "        how many times run generator updates per one discriminator update\n",
    "    -------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_frequency is not None:\n",
    "        saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "        \n",
    "    if load_dir is not None:\n",
    "        print(\"Reading checkpoints...\")\n",
    "        load(sess, load_dir)\n",
    "        print(\"Loaded checkpoints\")\n",
    "    else:\n",
    "        try:\n",
    "            tf.global_variables_initializer().run()\n",
    "        except:\n",
    "            tf.initialize_all_variables().run()\n",
    "\n",
    "    counter=1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        batch_idxs = min(len(data), np.inf) // BATCH_SIZE\n",
    "        for idx in range(0, batch_idxs):\n",
    "            batch_files = data[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "            batch = [get_image(batch_file, IMAGE_SIZE) for batch_file in batch_files]\n",
    "            batch_images = np.array(batch).astype(np.float32)\n",
    "            batch_z = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM]).astype(np.float32)\n",
    "\n",
    "            # Update D network\n",
    "            sess.run(d_optim, feed_dict={images: batch_images, z: batch_z,is_training: True})\n",
    "\n",
    "            # Update G network\n",
    "            for _ in range(n_generator_update):\n",
    "                sess.run(g_optim,\n",
    "                    feed_dict={z: batch_z, is_training: True})\n",
    "\n",
    "            errD_fake = d_loss_fake.eval({z: batch_z, is_training: False})\n",
    "            errD_real = d_loss_real.eval({images: batch_images, is_training: False})\n",
    "            errG = g_loss.eval({z: batch_z, is_training: False})\n",
    "\n",
    "            counter += 1\n",
    "            print(\"Epoch: [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}, g_loss: {:.8f}\".format(\n",
    "                epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG))\n",
    "\n",
    "            if np.mod(counter, save_frequency) == 1:\n",
    "                print(\"Saved model\")\n",
    "                saver.save(sess, \n",
    "                           os.path.join(save_dir, model_name))\n",
    "\n",
    "            if np.mod(counter, sample_frequency) == 1:\n",
    "                samples = sess.run(G, feed_dict={z: batch_z, is_training: False} )\n",
    "                save_images(samples, [MERGE_X, MERGE_Y],\n",
    "                            os.path.join(sample_dir, 'train_{:02d}_{:04d}.png'.format(epoch, idx)))\n",
    "                print (\"Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 218, 178, 3) for Tensor 'D/Placeholder:0', which has shape '(?, 64, 64, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bf67c455bcd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'checkpoint'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-c8fef6c2da40>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, load_dir, save_frequency, sample_frequency, sample_dir, save_dir, max_to_keep, model_name, n_epochs, n_generator_update)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# Update D network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_optim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_z\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Update G network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1111\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1112\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 218, 178, 3) for Tensor 'D/Placeholder:0', which has shape '(?, 64, 64, 3)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train(sess, save_dir='checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you generated something that looks like a face - it's cool! Add 2 points to your mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face interpolation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's interpolate between faces: generate two vectors $z_1$ and $z_2$ and get a batch of vectors of the form $\\alpha\\cdot z_1 + (1- \\alpha)\\cdot  z_2, \\alpha \\in [0,1].$ Generate faces on them and look at results. The generator displays pictures in the range from -1 to 1, so use the inverse transform function from the file utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchz = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a smile (1 point + 1 point for good results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's make face smiling. Find several vectors z, such that the generator generates smiling faces and not. Five vectors in every group should be enough (but the more, the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate \"smile vector\" as mean of vectors z with generated smile on it minus mean of vectors z with generated not smile on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the result of applying the smile vector: compare the results of generation before and after the addition of the smile vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If faces looks really cool, add bonus 1 point to your score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
